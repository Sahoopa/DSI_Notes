# Baysian AB Test

One of the most common applications of Bayesian analysis in industry is in A/B tests. In every major website you use it is likely that you have been part of an A/B test at some point and possibly even at any given time. A/B tests are essentially incrementally different versions of a product that are being tested on randomly selected users and evaluated on a performance metric. They are a good and widespread way to assess whether a proposed change will be beneficial, detrimental, or have no effect. To clarify, in an A/B test you would randomly assign some visitors to the site to one version of a page (group A) and some to another version (group B). Whichever performs better on your metric (click through rate, purchase, etc) then becomes the default. Gradually you hope to iterate towards a totally optimised webpage where every button is in the best position and every text as clear as it can be. Of course, this assumes that every customer responds the same way - there is a subdomain of such approaches where you actually show different categories of users different sites on an ongoing basis, but we will not get in to this and consider only the simpler (and more typical) case of which is the best overall version of a page/app.

We have already seen much of the code and logic for this, but here we apply it to the specific context and hopefully add greater clarity through practice. If you find, for example, that when you move to a role in industry that your company is doing its A/B testing in a frequentist framework, you might want to approach them to discuss whether performing a Bayesian A/B testing might give them greater access to insight and allow them to make clear conclusions with fewer test subjects (and hence less time). This case study could help give you a framework from which to do that.
